{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traveler DB\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This project takes i94 data and combines with city demographic data to populate a data mart on an AWS Redshift instance\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as sfunc\n",
    "from pyspark.sql.functions import udf, col, date_add, lit, to_date, when, sum as sql_sum, monotonically_increasing_id, upper, concat, substring\n",
    "from pyspark.sql.functions import sequence, dayofmonth, weekofyear, month, year, date_format\n",
    "from pyspark.sql import Row\n",
    "import boto3\n",
    "import psycopg2\n",
    "import os\n",
    "import shutil\n",
    "import configparser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "This project is taken from the perspective of a hotel company that is looking to gain insight into what kind of international travelers visit the cities and states they do business in so that they can better market their hotel to those visitors.  The i94 data used in conjunction with city demographics should provide insights into how to best market to travelers in existing markets for the hotel company, as well as to help generate a business strategy for expansion into new cities / markets.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "In this project, the i94 data is used in conjunction with the us-cities-demographics.csv file as well as various csv files cobbled together from the contents of the I94_SAS_Lables_Descriptions.SAS file.\n",
    "\n",
    "These dreived csv files include the following:\n",
    " * addr_codes.csv\n",
    " * countries.csv\n",
    " * ports.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    " * The demographic data contains a row for each different value in the column \"race\".  This will be flattened out to only one row per city and taking the sum of the \"Count\" column for each value.\n",
    " * There are many missing values for addr_code etc in the i94 data. These will be addressed by:\n",
    "  * Assigning dead-end foreign key values within the fact\n",
    "  * Supplying associated key values within respective dimensions\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean countries data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gather countries\n",
    "df_nation = spark.read.load(\"countries.csv\",\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "# add increasing id\n",
    "df_nation = df_nation.withColumn('idx', monotonically_increasing_id())\n",
    "\n",
    "# set increasing id to actually be 1-increments\n",
    "df_nation = df_nation.selectExpr(\"row_number() over (order by 'idx') as nation_key\",\n",
    "                                 \"COUNTRY_CODE as nation_code\", \"COUNTRY_DESCR as nation_descr\").dropDuplicates()\n",
    "\n",
    "# generate row to satisfy null fk references in fact\n",
    "nullrow_nation = df_nation.selectExpr(\"-999 as nation_key\",\n",
    "                                 \"-999 as nation_code\", \"'UNKNOWN' as nation_descr\").dropDuplicates()\n",
    "\n",
    "# union df and null row to prepare df for parquet write\n",
    "df_nation = df_nation.union(nullrow_nation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean addr_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather states & territories\n",
    "df_state_terr = spark.read.load(\"addr_codes.csv\",\n",
    "                               format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "# add increasing id\n",
    "df_state_terr = df_state_terr.withColumn('idx', monotonically_increasing_id())\n",
    "\n",
    "# set increasing id to actually be 1-increments\n",
    "df_state_terr = df_state_terr.selectExpr(\"row_number() over (order by 'idx') as state_terr_key\",\n",
    "                                        \"addr_code as state_terr_code\", \"addr_descr as state_terr_descr\").dropDuplicates()\n",
    "\n",
    "# generate row to satisfy null fk references in fact\n",
    "nullrow_state_terr = df_state_terr.selectExpr(\"-999 as state_terr_key\", \"'ZZ' as state_terr_code\", \"'UNKNOWN' as state_terr_descr\").dropDuplicates()\n",
    "\n",
    "# union df and null row to prepare df for parquet write\n",
    "df_state_terr = df_state_terr.union(nullrow_state_terr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean city demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather raw city demographic data\n",
    "df_city_demo = spark.read.load(\"us-cities-demographics.csv\",\n",
    "                     format=\"csv\", sep=\";\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "# rename the count column to something less \"reserved\"\n",
    "df_city_demo = df_city_demo.withColumnRenamed('Count','pop_count')\n",
    "\n",
    "# add columns for the demographic race/ethnicity elements\n",
    "df_city_demo = df_city_demo.withColumn('pop_african_american', when(df_city_demo.Race==\"Black or African-American\", df_city_demo.pop_count).otherwise(0)) \\\n",
    "    .withColumn('pop_hispanic_latino', when(df_city_demo.Race==\"Hispanic or Latino\", df_city_demo.pop_count).otherwise(0)) \\\n",
    "    .withColumn('pop_native_american', when(df_city_demo.Race==\"American Indian and Alaska Native\", df_city_demo.pop_count).otherwise(0)) \\\n",
    "    .withColumn('pop_asian', when(df_city_demo.Race==\"Asian\", df_city_demo.pop_count).otherwise(0)) \\\n",
    "    .withColumn('pop_white', when(df_city_demo.Race==\"White\", df_city_demo.pop_count).otherwise(0)) \\\n",
    "\n",
    "\n",
    "# sum the populations of the various race / ethnicity demographics by city\n",
    "df_city_demo = df_city_demo.groupBy(\"City\", \"State\", \"Median Age\", \"Male Population\", \"Female Population\", \"Total Population\", \"Number of Veterans\", \"Foreign-born\",\n",
    "                           \"Average Household Size\", \"State Code\").agg(sql_sum(\"pop_african_american\").alias(\"pop_african_american\"),\n",
    "                                                                       sql_sum(\"pop_hispanic_latino\").alias(\"pop_hispanic_latino\"),\n",
    "                                                                       sql_sum(\"pop_native_american\").alias(\"pop_native_american\"),\n",
    "                                                                       sql_sum(\"pop_asian\").alias(\"pop_asian\"), sql_sum(\"pop_white\").alias(\"pop_white\"))\n",
    "\n",
    "# set up a mapping dictionary to rename columns\n",
    "mapping = dict(zip(['City', 'State', 'State Code', 'Median Age', 'Average Household Size', 'Total Population', 'Male Population', 'Female Population', \n",
    "                    'Number of Veterans', 'Foreign-born', 'pop_african_american', 'pop_hispanic_latino', 'pop_native_american', 'pop_asian', 'pop_white'], \n",
    "                   ['city', 'state', 'state_code', 'median_age', 'avg_household_size', 'pop_total', 'pop_male', 'pop_female', \n",
    "                    'pop_veteran', 'pop_foreign_born', 'pop_african_american', 'pop_hispanic_latino', 'pop_native_american', 'pop_asian', 'pop_white']))\n",
    "\n",
    "# rename mapped columns\n",
    "df_city_demo = df_city_demo.select([col(c).alias(mapping.get(c, c)) for c in df_city_demo.columns])\n",
    "\n",
    "# inner join to df_state_terr to only get cities of us states / territories\n",
    "df_city_demo = df_city_demo.join(df_state_terr, (df_city_demo.state_code == df_state_terr.state_terr_code), 'inner')\n",
    "\n",
    "\n",
    "# add increasing id\n",
    "df_city_demo = df_city_demo.withColumn('idx', monotonically_increasing_id())\n",
    "\n",
    "# set increasing id to actually be 1-increments\n",
    "df_city_demo = df_city_demo.selectExpr(\"row_number() over (order by 'idx') as city_demo_key\", \"state_terr_key as city_state_terr_key\",\n",
    "                                        \"city as city_name\", \"state as state_name\", \"state_terr_code as state_code\", \n",
    "                                         \"cast(median_age as decimal(5,2)) as median_age\", \"cast(avg_household_size as decimal(5,2)) as avg_household_size\",\n",
    "                                         \"pop_total\", \"pop_male\", \"pop_female\", \"pop_foreign_born\", \"cast(pop_african_american as integer) as pop_african_american\",\n",
    "                                        \"cast(pop_hispanic_latino as integer) as pop_hispanic_latino\", \"cast(pop_native_american as integer) as pop_native_american\", \n",
    "                                       \"cast(pop_asian as integer) as pop_asian\", \"cast(pop_white as integer) as pop_white\")\n",
    "\n",
    "\n",
    "# generate row to satisfy null fk references in fact\n",
    "nullrow_city_demo = df_city_demo.selectExpr(\"-999 as city_demo_key\", \"-999 as city_state_terr_key\",\n",
    "                                                 \"'UNKNOWN' as city_name\", \"'UNKNOWN' as state_name\", \"'ZZ' as state_code\", \n",
    "                                         \"NULL as median_age\", \"NULL as avg_household_size\",\n",
    "                                         \"NULL as pop_total\", \"NULL as pop_male\", \"NULL as pop_female\", \"NULL as pop_foreign_born\", \"NULL as pop_african_american\",\n",
    "                                        \"NULL as pop_hispanic_latino\", \"NULL as pop_native_american\", \n",
    "                                       \"NULL as pop_asian\", \"NULL as pop_white\").dropDuplicates()\n",
    "\n",
    "# union df and null row to prepare df for parquet write\n",
    "df_city_demo = df_city_demo.union(nullrow_city_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean port data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather ports\n",
    "df_port = spark.read.load(\"ports.csv\",\n",
    "                               format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "df_port = df_port.withColumn('port_state_terr', substring(df_port.PORT_DESCR, -2, 2))\n",
    "\n",
    "\n",
    "# left join to df_state_terr to get ports of us states / territories (both on us_state_terr and the substring of the name due to false positive matches)\n",
    "df_port = df_port.join(df_state_terr, (df_port.port_state_terr == df_state_terr.state_terr_code) & (df_port.PORT_US_STATE_TERR == df_state_terr.state_terr_code), 'left')\n",
    "\n",
    "# generate port_state_terr_key column based on associated port_key or lack thereof\n",
    "df_port = df_port.withColumn('port_state_terr_key', when(df_port.state_terr_key.isNull()==1, -999).otherwise(df_port.state_terr_key)) \\\n",
    "\n",
    "# left join to df_city_demo to get cities with demographic information where applicable\n",
    "df_port = df_port.join(df_city_demo, (df_port.PORT_DESCR == concat(upper(df_city_demo.city_name), lit(', '), upper(df_city_demo.state_code))), 'left_outer')\n",
    "\n",
    "# generate port_city_demo_key column based on associated port_key or lack thereof\n",
    "df_port = df_port.withColumn('port_city_demo_key', when(df_port.city_demo_key.isNull()==1, -999).otherwise(df_port.city_demo_key)) \\\n",
    "\n",
    "# add increasing id\n",
    "df_port = df_port.withColumn('idx', monotonically_increasing_id())\n",
    "\n",
    "# set increasing id to actually be 1-increments\n",
    "df_port = df_port.selectExpr(\"row_number() over (order by 'idx') as port_key\", \"port_code as port_code\", \"port_descr as port_descr\",\n",
    "                                        \"state_terr_code as port_state_terr\", \"port_state_terr_key\", \"port_city_demo_key\").dropDuplicates()\n",
    "\n",
    "# generate row to satisfy null fk references in fact\n",
    "nullrow_port = df_port.selectExpr(\"-999 as port_key\", \"'ZZZ' as port_code\", \"'UNKNOWN' as port_descr\", \n",
    "                                  \"'ZZ' as port_state_terr\", \"-999 as port_state_terr_key\", \"-999 as port_city_demo_key\").dropDuplicates()\n",
    "\n",
    "# union df and null row to prepare df for parquet write\n",
    "df_port = df_port.union(nullrow_port)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate date dimension data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genrows(i):\n",
    "    \"\"\" generates a set number of rows \"\"\"\n",
    "    df_return = spark.createDataFrame([Row(date_num = 0)])\n",
    "    x = 1\n",
    "    while(x<i):\n",
    "        df_nextnum = spark.createDataFrame([Row(date_num = x)])\n",
    "        df_return = df_return.union(df_nextnum)\n",
    "        x=x+1\n",
    "    return df_return\n",
    "\n",
    "df_datenums = genrows(366) # here we generate 366 rows, one for each day in the year 2016\n",
    "\n",
    "\n",
    "df_bdate = spark.createDataFrame([Row(date_text = '2016-01-01')])\n",
    "df_bdate = df_bdate.withColumn('b_date', to_date(df_bdate.date_text))\n",
    "df_bdate = df_bdate.drop('date_text')\n",
    "\n",
    "df_date = df_datenums.crossJoin(df_bdate)\n",
    "df_date = df_date.selectExpr(\"*\", \"date_add(b_date, date_num) as date_key\")\n",
    "\n",
    "df_date = df_date.drop('date_num')\n",
    "df_date = df_date.drop('b_date')\n",
    "\n",
    "df_date = df_date.withColumn('date_day', dayofmonth(df_date.date_key))\\\n",
    "    .withColumn('date_week', weekofyear(df_date.date_key))\\\n",
    "    .withColumn('date_month', month(df_date.date_key))\\\n",
    "    .withColumn('date_year', year(df_date.date_key))\\\n",
    "    .withColumn('date_weekday', date_format(df_date.date_key, 'u'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "The data model is a modified star schema data warehouse model with an outrigger dimension for city demographics that does not tie to the fact table.\n",
    "\n",
    "The fact table is an aggregation of travelers by the intersection of the foreign keys, the travel date, and the traveler characteristics. Most characteristic columns are quasi-boolean smallint values such that they may be aggregated to get distinct counts of that characteristic by day.  The fact table is distributed by distkey of arrival_date, assuming that while there will be some days with more travelers than others, it will still be a pretty decent distribution candidate especially considering the expectation that most queries will be time-based.\n",
    "\n",
    "The dimensional tables are distributed to all nodes as they do not contain that much data and will not demand too much storage overhead.  their distribution to all nodes should help with query performance on joins.\n",
    "\n",
    "This model was chosen based on a use case of a hotel company's interest in how to market to various types of international travelers and how that may vary by city and/or state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"travelerdb_diagram.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the destination tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "configredshift = configparser.ConfigParser()\n",
    "configredshift.read_file(open('awscfg.cfg'))\n",
    "\n",
    "rshost = configredshift.get('CLUSTER','HOST')\n",
    "rsdbname = configredshift.get('CLUSTER','DB_NAME')\n",
    "rsuser = configredshift.get('CLUSTER','DB_USER')\n",
    "rspassword = configredshift.get('CLUSTER','DB_PASSWORD')\n",
    "rsport = configredshift.get('CLUSTER','DB_PORT')\n",
    "\n",
    "conn = psycopg2.connect(host=rshost, dbname=rsdbname, user=rsuser, password=rspassword, port=rsport)\n",
    "cur = conn.cursor()\n",
    "\n",
    "date_table_drop = \"drop table if exists date_dim\"\n",
    "nation_table_drop = \"drop table if exists nation_dim;\"\n",
    "state_terr_table_drop = \"drop table if exists state_terr_dim;\"\n",
    "port_table_drop = \"drop table if exists port_dim;\"\n",
    "city_demo_table_drop = \"drop table if exists city_demo_dim;\"\n",
    "traveler_table_drop = \"drop table if exists traveler_fact;\"\n",
    "\n",
    "drop_table_queries = [traveler_table_drop, date_table_drop, port_table_drop, city_demo_table_drop, state_terr_table_drop, nation_table_drop]\n",
    "\n",
    "date_table_create = \"\"\"create table date_dim(date_key date, date_day integer, date_week integer, date_month integer, date_year integer, date_weekday varchar,\n",
    "    primary key(date_key)) diststyle all;\"\"\"\n",
    "\n",
    "nation_table_create = \"\"\"create table nation_dim(nation_key integer, nation_code integer, nation_descr varchar,\n",
    "    primary key(nation_key)) diststyle all;\"\"\"\n",
    "\n",
    "state_terr_table_create = \"\"\"create table state_terr_dim(state_terr_key integer, state_terr_code varchar, state_terr_descr varchar,\n",
    "    primary key(state_terr_key)) diststyle all;\"\"\"\n",
    "\n",
    "city_demo_table_create = \"\"\"create table city_demo_dim(city_demo_key integer, city_state_terr_key integer, city_name varchar,\n",
    "    state_name varchar, state_code varchar, median_age decimal(5, 2), avg_household_size decimal(5,2), pop_total integer, pop_male integer, pop_female integer, pop_foreign_born integer,\n",
    "    pop_african_american integer, pop_hispanic_latino integer, pop_native_american integer, pop_asian integer, pop_white integer, primary key(city_demo_key),\n",
    "    foreign key(city_state_terr_key) references state_terr_dim(state_terr_key)) diststyle all;\"\"\"\n",
    "\n",
    "port_table_create = \"\"\"create table port_dim(port_key integer, port_code varchar, port_descr varchar, port_state_terr varchar, port_state_terr_key integer, port_city_demo_key integer,\n",
    "    primary key(port_key), foreign key(port_state_terr_key) references state_terr_dim(state_terr_key),\n",
    "    foreign key(port_city_demo_key) references city_demo_dim(city_demo_key)) diststyle all;\"\"\"\n",
    "\n",
    "traveler_table_create = \"\"\"create table traveler_fact(arrival_date date, citizen_nation_key integer, residence_nation_key integer, traveler_port_key integer,\n",
    "    traveler_port_state_terr_key integer, visiting_state_terr_key integer, gender_reported smallint, gender_male smallint, gender_female smallint, gender_other smallint, age_reported smallint,\n",
    "    age_0_11 smallint, age_12_17 smallint, age_18_24 smallint, age_25_34 smallint, age_35_44 smallint, age_45_54 smallint, age_55_64 smallint, age_65_74 smallint, age_75p smallint,\n",
    "    arrival_type_reported smallint, air_arrival smallint, sea_arrival smallint, land_arrival smallint, visa_type_reported smallint, business_visa smallint, pleasure_visa smallint,\n",
    "    student_visa smallint, traveler_count integer,\n",
    "    foreign key(citizen_nation_key) references nation_dim(nation_key), foreign key(residence_nation_key) references nation_dim(nation_key),\n",
    "    foreign key(traveler_port_key) references port_dim(port_key), foreign key(traveler_port_state_terr_key) references state_terr_dim(state_terr_key),\n",
    "    foreign key(visiting_state_terr_key) references state_terr_dim(state_terr_key),\n",
    "    foreign key(arrival_date) references date_dim(date_key)) distkey(arrival_date) ;\"\"\"\n",
    "\n",
    "\n",
    "create_table_queries = [date_table_create, nation_table_create, state_terr_table_create, city_demo_table_create, port_table_create, traveler_table_create]\n",
    "\n",
    "\n",
    "\n",
    "def drop_tables(cur, conn):\n",
    "    for query in drop_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "def create_tables(cur, conn):\n",
    "    for query in create_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "        \n",
    "drop_tables(cur, conn)\n",
    "\n",
    "create_tables(cur, conn)\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write dimensional dataframes to local parquet folders / files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditionally_remove_dir(path):\n",
    "    \"\"\" removes a directory and its contents \"\"\"\n",
    "    if os.path.isfile(path):\n",
    "        os.remove(path)\n",
    "    elif os.path.isdir(path):\n",
    "        shutil.rmtree(path)\n",
    "\n",
    "dframes = [\"df_date\", \"df_nation\", \"df_state_terr\", \"df_city_demo\", \"df_port\"]\n",
    "\n",
    "for dframe in dframes:\n",
    "    conditionally_remove_dir(dframe)\n",
    "    \n",
    "df_date.write.parquet(\"df_date\")\n",
    "df_nation.write.parquet(\"df_nation\")\n",
    "df_state_terr.write.parquet(\"df_state_terr\")\n",
    "df_city_demo.write.parquet(\"df_city_demo\")\n",
    "df_port.write.parquet(\"df_port\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create functions to upload parquet files to S3 and ingest to Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def upload_files(path):\n",
    "    \"\"\" uploads parquet files contained within a specified path to s3 \"\"\"\n",
    "    configupload = configparser.ConfigParser()\n",
    "    configupload.read_file(open('awscfg.cfg'))\n",
    "    \n",
    "    keyid = configupload.get('KEY_CREDS','KEY_ID')\n",
    "    keysecret = configupload.get('KEY_CREDS','KEY_SECRET')\n",
    "    s3region = configupload.get('S3','REGION')\n",
    "    s3bucket = configupload.get('S3','BUCKET')\n",
    "    \n",
    "    session = boto3.Session(\n",
    "        aws_access_key_id=keyid,\n",
    "        aws_secret_access_key=keysecret,\n",
    "        region_name=s3region\n",
    "    )\n",
    "    s3 = session.resource('s3')\n",
    "    bucket = s3.Bucket(s3bucket)\n",
    " \n",
    "    for subdir, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            full_path = os.path.join(subdir, file)\n",
    "            with open(full_path, 'rb') as data:\n",
    "                if full_path.endswith(\".parquet\"):\n",
    "                    bucket.put_object(Key=full_path[len(path)+1:], Body=data)\n",
    "                                   \n",
    "\n",
    "def get_parquet_files(path) :\n",
    "    \"\"\" generates a list of parquet file names for a given path \"\"\"\n",
    "    filelist = []\n",
    "    for subdir, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            full_path = os.path.join(subdir, file)\n",
    "            if full_path.endswith(\".parquet\"):\n",
    "                filelist.append(file)\n",
    "    return filelist\n",
    "\n",
    "\n",
    "def insert_to_redshift(parquetname, targetname):\n",
    "    \"\"\" copies parquet files of a specified parquet from s3 to specified redshift table \"\"\"\n",
    "    configredshift = configparser.ConfigParser()\n",
    "    configredshift.read_file(open('awscfg.cfg'))\n",
    "    \n",
    "    rshost = configredshift.get('CLUSTER','HOST')\n",
    "    rsdbname = configredshift.get('CLUSTER','DB_NAME')\n",
    "    rsuser = configredshift.get('CLUSTER','DB_USER')\n",
    "    rspassword = configredshift.get('CLUSTER','DB_PASSWORD')\n",
    "    rsport = configredshift.get('CLUSTER','DB_PORT')\n",
    "    bucketsource = configredshift.get('S3','BUCKET')\n",
    "    bucketiam = configredshift.get('IAM_ROLE','ARN')\n",
    "\n",
    "    conn = psycopg2.connect(host=rshost, dbname=rsdbname, user=rsuser, password=rspassword, port=rsport)\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    filestoget = get_parquet_files(parquetname)\n",
    "    \n",
    "    for parquetfile in filestoget:\n",
    "\n",
    "        copycommand = (\"\"\" COPY {}\n",
    "        FROM 's3://{}/{}'\n",
    "        IAM_ROLE '{}'\n",
    "        FORMAT AS PARQUET;\n",
    "        \"\"\").format(targetname, bucketsource, parquetfile, bucketiam)\n",
    "        \n",
    "        cur.execute(copycommand)\n",
    "        conn.commit()\n",
    "        \n",
    "\n",
    "def delete_parquets(path):\n",
    "    \"\"\" deletes parquet files from s3 \"\"\"\n",
    "    configupload = configparser.ConfigParser()\n",
    "    configupload.read_file(open('awscfg.cfg'))\n",
    "    \n",
    "    delkeyid = configupload.get('KEY_CREDS','KEY_ID')\n",
    "    delkeysecret = configupload.get('KEY_CREDS','KEY_SECRET')\n",
    "    dels3region = configupload.get('S3','REGION')\n",
    "    dels3bucket = configupload.get('S3','BUCKET')\n",
    "    \n",
    "    session = boto3.Session(\n",
    "        aws_access_key_id=delkeyid,\n",
    "        aws_secret_access_key=delkeysecret,\n",
    "        region_name=dels3region\n",
    "    )\n",
    "    s3 = session.resource('s3')\n",
    "    bucket = s3.Bucket(dels3bucket)\n",
    "    parquetstodel = get_parquet_files(path)\n",
    "    \n",
    "    for parquet in parquetstodel:\n",
    "        obj = s3.Object(dels3bucket, parquet)\n",
    "        obj.delete()\n",
    "        \n",
    "def get_row_count(tablename):\n",
    "    \"\"\" retrieves the row count of a specified table \"\"\"\n",
    "    configredshift = configparser.ConfigParser()\n",
    "    configredshift.read_file(open('awscfg.cfg'))\n",
    "\n",
    "    rshost = configredshift.get('CLUSTER','HOST')\n",
    "    rsdbname = configredshift.get('CLUSTER','DB_NAME')\n",
    "    rsuser = configredshift.get('CLUSTER','DB_USER')\n",
    "    rspassword = configredshift.get('CLUSTER','DB_PASSWORD')\n",
    "    rsport = configredshift.get('CLUSTER','DB_PORT')\n",
    "\n",
    "    conn = psycopg2.connect(host=rshost, dbname=rsdbname, user=rsuser, password=rspassword, port=rsport)\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    rowcountsql = (\"SELECT COUNT(*) FROM {};\").format(tablename)\n",
    "    \n",
    "    cur.execute(rowcountsql)\n",
    "    \n",
    "    sqlresult = cur.fetchone()\n",
    "    tablecount = sqlresult[0]\n",
    "    \n",
    "    return tablecount\n",
    "\n",
    "\n",
    "def get_parquet_count(dfname):\n",
    "    \"\"\" retrieves the row count of a specified parquet file \"\"\"\n",
    "    parquetdf = spark.read.parquet(dfname)\n",
    "    parquetcount = parquetdf.count()\n",
    "    return parquetcount\n",
    "\n",
    "def verify_counts(dfname, tablename, initct):\n",
    "    \"\"\" compares the row counts of the target table to the sum of the row counts of the parquet and the row count of the target prior to load \"\"\"\n",
    "    rowct = get_row_count(tablename)\n",
    "    pqct = get_parquet_count(dfname)\n",
    "    \n",
    "    if rowct != (pqct+initct):\n",
    "        raise ValueError(\"row count in table {} does not match previous table row count plus row count in parquet folder {}.\".format(tablename, dfname))\n",
    "\n",
    "\n",
    "def remove_dir(path):\n",
    "    \"\"\" removes files and the directory of the specified path \"\"\"\n",
    "    if os.path.isfile(path):\n",
    "        os.remove(path)  # remove the file\n",
    "    elif os.path.isdir(path):\n",
    "        shutil.rmtree(path)  # remove dir and all contents\n",
    "    else:\n",
    "        raise ValueError(\"file {} is not a file or dir.\".format(path))\n",
    "            \n",
    "\n",
    "            \n",
    "def upload_dataframe(dfname, tablename):\n",
    "    \n",
    "    prevcount = get_row_count(tablename)\n",
    "    upload_files(dfname)\n",
    "    insert_to_redshift(dfname, tablename)\n",
    "    delete_parquets(dfname)\n",
    "    verify_counts(dfname, tablename, prevcount)\n",
    "    remove_dir(dfname)\n",
    "            \n",
    " \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload dimension data to S3 and Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dims():\n",
    "    df_table_map = dict(zip([\"df_date\", \"df_nation\", \"df_state_terr\", \"df_city_demo\", \"df_port\"],[\"date_dim\", \"nation_dim\", \"state_terr_dim\", \"city_demo_dim\", \"port_dim\"]))\n",
    "    for (key, value) in df_table_map.items():\n",
    "        upload_dataframe(key, value)\n",
    "        \n",
    "\n",
    "process_dims()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create functions to clean I94 Data and write to parquet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_month(monthno):\n",
    "    \"\"\" associates a 3 char string representing a month with that month's number value \"\"\"\n",
    "    month_map = dict(zip([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']))\n",
    "    for (key, value) in month_map.items():\n",
    "        if key==monthno:\n",
    "            return value\n",
    "        \n",
    "def remove_fact_dir(path):\n",
    "    \"\"\" removes files and a directory for a given path \"\"\"\n",
    "    if os.path.isfile(path):\n",
    "        os.remove(path)\n",
    "    elif os.path.isdir(path):\n",
    "        shutil.rmtree(path)\n",
    "\n",
    "\n",
    "def clean_fact(runmonth):\n",
    "    \"\"\" cleans the i94 data to fit to the target table structure \"\"\"\n",
    "\n",
    "    curmonth_char = get_month(runmonth)\n",
    "\n",
    "    location_prefix = '../../data/18-83510-I94-Data-2016/i94_'\n",
    "    location_suffix = '16_sub.sas7bdat'\n",
    "\n",
    "    data_location = location_prefix + curmonth_char + location_suffix\n",
    "\n",
    "    df_nation_cit = df_nation.selectExpr(\"nation_key as nation_key_cit\", \"nation_descr as nation_descr_cit\", \"nation_code as nation_code_cit\")\n",
    "    df_nation_res = df_nation.selectExpr(\"nation_key as nation_key_res\", \"nation_descr as nation_descr_res\", \"nation_code as nation_code_res\")\n",
    "\n",
    "\n",
    "    df_fact = spark.read.format('com.github.saurfang.sas.spark').load(data_location)\n",
    "\n",
    "    df_fact = df_fact.withColumn('arrdate_int', df_fact.arrdate.cast('integer')) \\\n",
    "        .withColumn('diff_date', to_date(lit('1960-01-01'))) \\\n",
    "        .withColumn('gender_reported', when(df_fact.gender.isNotNull()==1, 1).otherwise(0)) \\\n",
    "        .withColumn('gender_male', when(df_fact.gender==\"M\", 1).otherwise(0)) \\\n",
    "        .withColumn('gender_female', when(df_fact.gender==\"F\", 1).otherwise(0)) \\\n",
    "        .withColumn('gender_other', when(df_fact.gender==\"M\", 0).when(df_fact.gender==\"F\", 0).when(df_fact.gender.isNull()==1, 0).otherwise(1)) \\\n",
    "        .withColumn('age_reported', when(df_fact.i94bir.isNotNull()==1, 1).otherwise(0)) \\\n",
    "        .withColumn('age_0_11', when(df_fact.i94bir <= 11, 1).otherwise(0)) \\\n",
    "        .withColumn('age_12_17', when(df_fact.i94bir.between(12, 17)==1, 1).otherwise(0)) \\\n",
    "        .withColumn('age_18_24', when(df_fact.i94bir.between(18, 24)==1, 1).otherwise(0)) \\\n",
    "        .withColumn('age_25_34', when(df_fact.i94bir.between(25, 34)==1, 1).otherwise(0)) \\\n",
    "        .withColumn('age_35_44', when(df_fact.i94bir.between(35, 44)==1, 1).otherwise(0)) \\\n",
    "        .withColumn('age_45_54', when(df_fact.i94bir.between(45, 54)==1, 1).otherwise(0)) \\\n",
    "        .withColumn('age_55_64', when(df_fact.i94bir.between(55, 64)==1, 1).otherwise(0)) \\\n",
    "        .withColumn('age_65_74', when(df_fact.i94bir.between(65, 74)==1, 1).otherwise(0)) \\\n",
    "        .withColumn('age_75p', when(df_fact.i94bir >= 75, 1).otherwise(0)) \\\n",
    "        .withColumn('arrival_type_reported', when(df_fact.i94mode.isNull()==1, 0).when(df_fact.i94mode==9, 0).otherwise(1)) \\\n",
    "        .withColumn('air_arrival', when(df_fact.i94mode ==1, 1).otherwise(0)) \\\n",
    "        .withColumn('sea_arrival', when(df_fact.i94mode ==2, 1).otherwise(0)) \\\n",
    "        .withColumn('land_arrival', when(df_fact.i94mode ==3, 1).otherwise(0)) \\\n",
    "        .withColumn('visa_type_reported', when(df_fact.i94visa ==1, 1).when(df_fact.i94visa ==2, 1).when(df_fact.i94visa ==3, 1).otherwise(0)) \\\n",
    "        .withColumn('business_visa', when(df_fact.i94visa ==1, 1).otherwise(0)) \\\n",
    "        .withColumn('pleasure_visa', when(df_fact.i94visa ==2, 1).otherwise(0)) \\\n",
    "        .withColumn('student_visa', when(df_fact.i94visa ==3, 1).otherwise(0)) \\\n",
    "\n",
    "    # add column arrival_date as diff_date plus arrdate_int \n",
    "    df_fact = df_fact.selectExpr(\"*\", \"date_add(diff_date, arrdate_int) as arrival_date\", \"cast(count as int) as traveler_count\", \"cast(i94cit as int) as i94_cit\",\n",
    "                              \"cast(i94res as int) as i94_res\", \"i94port as i94_port\", \"i94addr as i94_addr\")\n",
    "\n",
    "    df_fact = df_fact.select([df_fact.arrival_date, df_fact.i94_cit, df_fact.i94_res, df_fact.i94_port, df_fact.gender_reported, df_fact.gender_male, \n",
    "                            df_fact.gender_female, df_fact.gender_other, df_fact.age_reported, df_fact.age_0_11, df_fact.age_12_17, df_fact.age_18_24,\n",
    "                            df_fact.age_25_34, df_fact.age_35_44, df_fact.age_45_54, df_fact.age_55_64, df_fact.age_65_74, df_fact.age_75p,\n",
    "                            df_fact.arrival_type_reported, df_fact.air_arrival, df_fact.sea_arrival, df_fact.land_arrival, \n",
    "                            df_fact.visa_type_reported, df_fact.business_visa, df_fact.pleasure_visa, df_fact.student_visa, \n",
    "                            df_fact.i94_addr, df_fact.traveler_count ])\n",
    "\n",
    "    # left join to df_nation_cit to get nation_key of i94cit\n",
    "    df_fact = df_fact.join(df_nation_cit, (df_fact.i94_cit == df_nation_cit.nation_code_cit), 'left_outer')\n",
    "\n",
    "    # left join to df_nation_res to get nation_key of i94res\n",
    "    df_fact = df_fact.join(df_nation_res, (df_fact.i94_res == df_nation_res.nation_code_res), 'left_outer')\n",
    "\n",
    "    # left join to df_port to get port_key and port_state_terr_key of i94port\n",
    "    df_fact = df_fact.join(df_port, (df_fact.i94_port == df_port.port_code), 'left_outer')\n",
    "\n",
    "    # left join to df_state_terr to get visiting_state_terr_key of i94addr\n",
    "    df_fact = df_fact.join(df_state_terr, (df_fact.i94_addr == df_state_terr.state_terr_code), 'left_outer')\n",
    "\n",
    "    # Set foreign key values\n",
    "    df_fact = df_fact.withColumn('citizen_nation_key', when(df_fact.nation_key_cit.isNull()==1, -999).otherwise(df_fact.nation_key_cit)) \\\n",
    "        .withColumn('residence_nation_key', when(df_fact.nation_key_res.isNull()==1, -999).otherwise(df_fact.nation_key_res)) \\\n",
    "        .withColumn('traveler_port_key', when(df_fact.port_key.isNull()==1, -999).otherwise(df_fact.port_key)) \\\n",
    "        .withColumn('traveler_port_state_terr_key', when(df_fact.port_key.isNull()==1, -999).otherwise(df_fact.port_state_terr_key)) \\\n",
    "        .withColumn('visiting_state_terr_key', when(df_fact.state_terr_key.isNull()==1, -999).otherwise(df_fact.state_terr_key)) \\\n",
    "\n",
    "    # Group by date, foreign keys and traveler characteristics\n",
    "    df_fact = df_fact.groupBy(\"arrival_date\", \"citizen_nation_key\", \"residence_nation_key\", \"traveler_port_key\", \"traveler_port_state_terr_key\", \n",
    "                                \"gender_reported\", \"gender_male\", \"gender_female\", \"gender_other\",\n",
    "                               \"age_reported\", \"age_0_11\", \"age_12_17\", \"age_18_24\", \"age_25_34\",\"age_35_44\", \"age_45_54\", \"age_55_64\",\"age_65_74\", \"age_75p\",\n",
    "                               \"arrival_type_reported\", \"air_arrival\", \"sea_arrival\", \"land_arrival\", \"visa_type_reported\",\n",
    "                               \"business_visa\", \"pleasure_visa\",\"student_visa\", \"visiting_state_terr_key\").agg(sql_sum(\"traveler_count\").alias(\"traveler_count\"))\n",
    "\n",
    "    # set columns for parquet write\n",
    "    df_fact = df_fact.selectExpr(\"arrival_date\", \"citizen_nation_key\", \"residence_nation_key\", \"traveler_port_key\", \"traveler_port_state_terr_key\", \n",
    "                               \"visiting_state_terr_key\", \"cast(gender_reported as smallint) as gender_reported\", \"cast(gender_male as smallint) as gender_male\", \n",
    "                                 \"cast(gender_female as smallint) as gender_female\", \"cast(gender_other as smallint) as gender_other\",\n",
    "                               \"cast(age_reported as smallint) as age_reported\", \"cast(age_0_11 as smallint) as age_0_11\", \"cast(age_12_17 as smallint) as age_12_17\", \n",
    "                                 \"cast(age_18_24 as smallint) as age_18_24\", \"cast(age_25_34 as smallint) as age_25_34\",\"cast(age_35_44 as smallint) as age_35_44\", \n",
    "                                 \"cast(age_45_54 as smallint) as age_45_54\", \"cast(age_55_64 as smallint) as age_55_64\",\"cast(age_65_74 as smallint) as age_65_74\", \n",
    "                                 \"cast(age_75p as smallint) as age_75p\", \"cast(arrival_type_reported as smallint) as arrival_type_reported\", \n",
    "                                 \"cast(air_arrival as smallint) as air_arrival\", \"cast(sea_arrival as smallint) as sea_arrival\", \"cast(land_arrival as smallint) as land_arrival\", \n",
    "                                 \"cast(visa_type_reported as smallint) as visa_type_reported\", \"cast(business_visa as smallint) as business_visa\", \n",
    "                                 \"cast(pleasure_visa as smallint) as pleasure_visa\",  \"cast(student_visa as smallint) as student_visa\",\"cast(traveler_count as int) as traveler_count\")\n",
    "    \n",
    "    remove_fact_dir(\"df_fact\")\n",
    "    df_fact.write.parquet(\"df_fact\")\n",
    "    \n",
    "\n",
    "def process_fact(runmonth):\n",
    "    clean_fact(runmonth)\n",
    "    upload_dataframe(\"df_fact\", \"traveler_fact\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload fact data to S3 and Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_month = 1\n",
    "end_month = 2\n",
    "\n",
    "months_to_run = list(range(start_month, end_month+1))\n",
    "\n",
    "for month in months_to_run:\n",
    "    process_fact(month)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "The project includes two methods for maintaining data quality:\n",
    " * Implementation of multiple foreign key constraints\n",
    " * Row count checks on each load\n",
    " \n",
    "The implementaion of the primary key / foreign key relationships is illustrated within the image in section 3.1\n",
    "\n",
    "Row count checks are implemented within the <b>upload_dataframe</b> function.  This is because the fact table load is built to run in monthly batches due to the amount of data and the way the data is retrieved from its source.  The data quality check must be embedded within the load mechanism due to the incremental nature of the load.\n",
    "\n",
    " * First, a row count is retrieved from the table prior to the table being loaded.\n",
    " * Once the table is loaded, a new row count is retrieved from the target table.\n",
    " * Once that count is retrieved, a count of rows from the local parquet files is retrieved.\n",
    " * Finally, the row count of the target table is compared to the sum of the count from the local parquet file and the count of rows in the table prior to the load."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n",
    " *  The data dictionary is available in the project workspace and can be found in the file <b>travelerdb_data_dictionary.txt</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "##### Choice of Technologies / Tools\n",
    " * Spark and Redshift were chosen as the key tools for the project. This is due to the following:\n",
    "  * Spark offers an ideal environment for processing the i94 data considering its size\n",
    "  * The data lends itself well to a Redshift as it can easily be broken out into Fact and Dimension elements\n",
    "  * The scale and shape of the data does not exceed the limitations of a parallel-processed data warehousing technology\n",
    "\n",
    "##### Propose how often the data should be updated and why.\n",
    "##### Data Update Cadence\n",
    " * At its most frequent, this data could be updated on a monthly basis. This is due to the source data being grouped by month.\n",
    " * In reality, it appears the i94 data is really only made available in yearly chunks. One could realisitically not expect new data to arrive except once per year.\n",
    "\n",
    "##### Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x:\n",
    "  * I believe Spark + Redshift could still make for a good choice, though it would require either going to 16 nodes or upgrading to a ds2.xlarge instance.\n",
    "  * There would be a significant increase in s3 costs that would have to be considered.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "  * Assuming the hypothetical that new data arrived daily, it would probably be best to implement an airflow solution that would govern the implementation of the spark jobs.\n",
    "  * One would need to gain understanding of when upstream data was made available each day, as well as get a baseline for how long the DAG takes to complete.\n",
    "  * Given the knowledge of the above elements as well as the SLA, one could determine to run with either an availability-driven or deadline-driven schedule\n",
    " * The database needed to be accessed by 100+ people.\n",
    "  * I believe Redshift would still be fine here, but it may be necesary to configure WLM queues to manage the query volume  or enable concurrency scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dir(path):\n",
    "    \"\"\" param <path> could either be relative or absolute. \"\"\"\n",
    "    if os.path.isfile(path):\n",
    "        os.remove(path)  # remove the file\n",
    "    elif os.path.isdir(path):\n",
    "        shutil.rmtree(path)  # remove dir and all contains\n",
    "    else:\n",
    "        raise ValueError(\"file {} is not a file or dir.\".format(path))\n",
    "        \n",
    "remove_dir('df_fact')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
